{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a36123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# Third-party library imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ca9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is needed for all the models\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "def tokenize_english(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\.\\,\\!\\?\\:\\;\\’\\'\\-]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if any(c.isalnum() for c in t)]\n",
    "    return tokens\n",
    "\n",
    "with open('data/annotations/captions_train2017.json', 'r') as f:\n",
    "    coco_train = json.load(f)\n",
    "\n",
    "with open('data/annotations/captions_val2017.json', 'r') as f:\n",
    "    coco_val = json.load(f)\n",
    "\n",
    "img_id_to_filename = {img['id']: img['file_name'] for img in coco_train['images']}\n",
    "\n",
    "annotations = []\n",
    "for ann in coco_train['annotations']:\n",
    "    fname = img_id_to_filename[ann['image_id']]\n",
    "    tokens = tokenize_english(ann['caption'])\n",
    "    seq = ['<start>'] + tokens + ['<end>']\n",
    "    annotations.append((fname, seq))\n",
    "\n",
    "\n",
    "min_freq = 5\n",
    "counter = Counter(tok for _, seq in annotations for tok in seq)\n",
    "words = [w for w, cnt in counter.items() if cnt >= min_freq]\n",
    "specials = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "itos = specials + words\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab = {'itos': itos, 'stoi': stoi}\n",
    "\n",
    "numerical = []\n",
    "for fname, seq in annotations:\n",
    "    ids = [stoi.get(tok, stoi['<unk>']) for tok in seq]\n",
    "    numerical.append((fname, ids))\n",
    "\n",
    "\n",
    "img_id_to_filename_val = {img['id']: img['file_name'] for img in coco_val['images']}\n",
    "\n",
    "annotations_val = []\n",
    "for ann in coco_val['annotations']:\n",
    "    fname = img_id_to_filename_val[ann['image_id']]\n",
    "    tokens = tokenize_english(ann['caption'])\n",
    "    seq = ['<start>'] + tokens + ['<end>']\n",
    "    annotations_val.append((fname, seq))\n",
    "\n",
    "numerical_val = []\n",
    "for fname, seq in annotations_val:\n",
    "    ids = [stoi.get(tok, stoi['<unk>']) for tok in seq]\n",
    "    numerical_val.append((fname, ids))\n",
    "\n",
    "\n",
    "class CaptionImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, vocab, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, seq_ids = self.annotations[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(seq_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, seqs = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    lengths = [len(s) for s in seqs]\n",
    "    max_len = max(lengths)\n",
    "    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :lengths[i]] = s\n",
    "    return imgs, padded, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26e7db",
   "metadata": {},
   "source": [
    "# 1. USING TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfef3b8",
   "metadata": {},
   "source": [
    "## 1.1 NO PRETRAINED CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b53d90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset that will only be used for computing mean and std\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, fname)\n",
    "            for fname in os.listdir(img_dir)\n",
    "            if fname.lower().endswith(('.jpg', '.png'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Function to compute mean and std\n",
    "def compute_mean_std(img_dir, batch_size=64, num_workers=8):\n",
    "    basic_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset_mean = SimpleImageDataset(img_dir, transform=basic_transform)\n",
    "    loader_mean = DataLoader(dataset_mean, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=num_workers,\n",
    "                        pin_memory=True)\n",
    "\n",
    "    sum_rgb = torch.zeros(3)\n",
    "    sum_squared = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    for batch in loader_mean:\n",
    "        b, c, h, w = batch.shape\n",
    "        sum_rgb += batch.sum(dim=[0, 2, 3])\n",
    "        sum_squared += (batch ** 2).sum(dim=[0, 2, 3])\n",
    "        num_pixels += b * h * w\n",
    "\n",
    "    mean = sum_rgb / num_pixels\n",
    "    std = torch.sqrt((sum_squared / num_pixels) - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "# Compute mean and std for the training images and create the datasets and loaders\n",
    "img_directory = 'data/images/train2017'\n",
    "if os.path.exists('image_stats.pkl'):\n",
    "    with open('image_stats.pkl', 'rb') as f:\n",
    "        stats = pickle.load(f)\n",
    "        mean, std = stats['mean'], stats['std']\n",
    "else:\n",
    "    mean, std = compute_mean_std(img_directory)\n",
    "    stats = {'mean': mean, 'std': std}\n",
    "    with open('image_stats.pkl', 'wb') as f:\n",
    "        pickle.dump(stats, f)\n",
    "\n",
    "train_transform_no_pretrained = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(\n",
    "        256, \n",
    "        scale=(0.9, 1.0), \n",
    "        ratio=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.1, \n",
    "        contrast=0.1, \n",
    "        saturation=0.1, \n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "dataset_no_pretrained = CaptionImageDataset(\n",
    "    img_dir=img_directory,\n",
    "    annotations=numerical,\n",
    "    vocab=vocab,\n",
    "    transform=train_transform_no_pretrained\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader_no_pretrained = DataLoader(\n",
    "    dataset_no_pretrained,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_transform_no_pretrained = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "val_img_directory = 'data/images/val2017'\n",
    "\n",
    "val_dataset_no_pretrained = CaptionImageDataset(\n",
    "    img_dir=val_img_directory,\n",
    "    annotations=numerical_val,\n",
    "    vocab=vocab,\n",
    "    transform=val_transform_no_pretrained\n",
    ")\n",
    "\n",
    "val_loader_no_pretrained = DataLoader(\n",
    "    val_dataset_no_pretrained,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Define the models\n",
    "# Encoder\n",
    "class SimpleEncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SimpleEncoderCNN, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)  # 32×32\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))  # reduce a 1×1\n",
    "        )\n",
    "        self.embed = nn.Linear(512, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embed(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "    \n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_dim, vocab_size, max_len=70, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_size),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def _generate_square_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = captions[:, :-1]  \n",
    "        x = self.embed(tgt) * math.sqrt(self.embed.embedding_dim)\n",
    "        x = self.pos_enc(x)\n",
    "        x = x.transpose(0, 1) \n",
    "\n",
    "        mask = self._generate_square_mask(x.size(0), x.device)\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        mem = features.unsqueeze(0)           \n",
    "        cross_out, _ = self.cross_attn(x, mem, mem)\n",
    "        x = self.norm2(x + self.dropout(cross_out))\n",
    "\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "\n",
    "        x = x.transpose(0, 1)      \n",
    "        logits = self.out(x)                                \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4480b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/12] Train:   0%|          | 9/9012 [00:42<11:49:30,  4.73s/it, lastLoss=9.3566, lr=3.48e-10, meanLoss=9.3717]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m     56\u001b[0m     feats   \u001b[38;5;241m=\u001b[39m encoder(images)\n\u001b[0;32m---> 57\u001b[0m     logits  \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     60\u001b[0m     logits\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size),\n\u001b[1;32m     61\u001b[0m     captions[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 212\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m    209\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(x)\n\u001b[1;32m    210\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m--> 212\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_square_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, attn_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m    214\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_out))\n",
      "Cell \u001b[0;32mIn[14], line 203\u001b[0m, in \u001b[0;36mTransformerDecoder._generate_square_mask\u001b[0;34m(self, sz, device)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_square_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, sz, device):\n\u001b[0;32m--> 203\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "embed_size  = 512\n",
    "num_heads   = 8\n",
    "hidden_dim  = 2048\n",
    "num_epochs = 12\n",
    "lr = 7e-3\n",
    "weight_decay = 1e-4\n",
    "warmup_steps = 40000\n",
    "\n",
    "vocab_size  = len(vocab['itos'])\n",
    "#Load the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Model\n",
    "encoder = SimpleEncoderCNN(embed_size).to(device)\n",
    "decoder = TransformerDecoder(\n",
    "    embed_size, num_heads, hidden_dim, vocab_size\n",
    ").to(device)\n",
    "\n",
    "#Loss function, optimizer and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['stoi']['<pad>'])\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "def get_transformer_scheduler(optimizer, d_model, warmup_steps=1000):\n",
    "    def lr_lambda(step):\n",
    "        step = max(step, 1)\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "scaler = torch.amp.GradScaler(\n",
    "    'cuda',\n",
    "    init_scale=512, \n",
    "    growth_interval=2000,\n",
    "    growth_factor=2.0,\n",
    "    backoff_factor=0.5\n",
    ")\n",
    "scheduler = get_transformer_scheduler(optimizer, embed_size, warmup_steps=warmup_steps)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader_no_pretrained, desc=f\"[Epoch {epoch}/{num_epochs}] Train\")\n",
    "    for images, captions, lengths in pbar:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            feats   = encoder(images)\n",
    "            logits  = decoder(feats, captions)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.float().view(-1, vocab_size),\n",
    "            captions[:, 1:].contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            max_norm=0.5\n",
    "        )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix(lastLoss=f\"{loss.item():.4f}\", meanLoss=f\"{train_loss/(pbar.n+1):.4f}\", lr=f\"{lr:.2e}\")\n",
    "\n",
    "    \n",
    "    avg_train = train_loss / len(train_loader_no_pretrained)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    val_loss = 0.0\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in tqdm(val_loader_no_pretrained, desc=f\"[Epoch {epoch}/{num_epochs}] Val  \"):\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                feats   = encoder(images)\n",
    "                outputs = decoder(feats, captions)\n",
    "                loss    = criterion(\n",
    "                    outputs.view(-1, vocab_size),\n",
    "                    captions[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val = val_loss / len(val_loader_no_pretrained)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {avg_train:.4f}, Val Loss = {avg_val:.4f}\")\n",
    "\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'encoder_state': encoder.state_dict(),\n",
    "            'decoder_state': decoder.state_dict(),\n",
    "            'optim_state': optimizer.state_dict(),\n",
    "            'sched_state': scheduler.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(ckpt, 'best_model_no_pretrained.pth')\n",
    "        print(f\"best_model_no_pretrained updated (Val Loss: {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adc92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        token1   prob1     token2   prob2 token3   prob3     token4   prob4      token5   prob5  token6   prob6      token7   prob7  token8   prob8  token9   prob9  token10  prob10\n",
      "pos                                                                                                                                                                                 \n",
      "1            a  0.7109        the  0.0495    two  0.0476         an  0.0287       there  0.0186  people  0.0163     several  0.0108   three  0.0102     man  0.0067     some  0.0063\n",
      "2          man  0.2203     person  0.1730  group  0.0389     surfer  0.0347       large  0.0317   woman  0.0268       skier  0.0208   small  0.0199  couple  0.0148    plane  0.0137\n",
      "3       riding  0.1493         in  0.1136     is  0.1118         on  0.0695      flying  0.0485    with  0.0350    standing  0.0319     and  0.0245    that  0.0240  wearing  0.0233\n",
      "4            a  0.5615       skis  0.0991     on  0.0655        the  0.0308        down  0.0191      an  0.0178         his  0.0104      in  0.0101   water  0.0070       up  0.0052\n",
      "5         wave  0.1815  surfboard  0.1328   kite  0.0459  snowboard  0.0337  skateboard  0.0331   beach  0.0291        surf  0.0287    snow  0.0277   snowy  0.0185   person  0.0158\n",
      "6           on  0.3123         in  0.2129  <end>  0.1632       with  0.0224       while  0.0167       a  0.0140        down  0.0139  riding  0.0115    near  0.0100       at  0.0087\n",
      "7            a  0.5366        the  0.2438    top  0.0476       skis  0.0291         his  0.0171      it  0.0114          an  0.0094   their  0.0052    some  0.0050    water  0.0046\n",
      "8    surfboard  0.1738      beach  0.1451   wave  0.1385      snowy  0.0382   snowboard  0.0309    surf  0.0296  skateboard  0.0274    kite  0.0246    snow  0.0225     hill  0.0188\n",
      "9        <end>  0.5161         in  0.1899     on  0.1033       with  0.0187          at  0.0130    near  0.0091         and  0.0087       a  0.0058   while  0.0055   riding  0.0050\n",
      "a man riding a wave on a surfboard\n"
     ]
    }
   ],
   "source": [
    "#Data for the reconstruction of the model\n",
    "itos = vocab['itos']\n",
    "stoi = vocab['stoi']\n",
    "pad_idx   = stoi['<pad>']\n",
    "start_idx = stoi['<start>']\n",
    "end_idx   = stoi['<end>']\n",
    "embed_size = 512\n",
    "num_heads  = 8\n",
    "hidden_dim = 2048\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Reconstruction of the model\n",
    "encoder = SimpleEncoderCNN(embed_size).to(device)\n",
    "decoder = TransformerDecoder(\n",
    "    embed_size, num_heads, hidden_dim, len(itos)\n",
    ").to(device)\n",
    "ckpt = torch.load('best_model_no_pretrained.pth', map_location=device)\n",
    "encoder.load_state_dict(ckpt['encoder_state'])\n",
    "decoder.load_state_dict(ckpt['decoder_state'])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "infer_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Function to caption an image and print the results in a table\n",
    "@torch.no_grad()\n",
    "def caption_with_table(img_path, max_len=50, top_k=10, temperature=1.0):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = infer_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.amp.autocast(device.type, torch.float16):\n",
    "        feat = encoder(image)  \n",
    "\n",
    "    steps = [] \n",
    "    caption_idx = [start_idx]\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        seq_in = caption_idx + [pad_idx]\n",
    "        tgt    = torch.tensor(seq_in, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.amp.autocast(device.type, torch.float16):\n",
    "            logits = decoder(feat, tgt)\n",
    "\n",
    "        logits_step = logits[0, -1] / temperature\n",
    "        probs       = torch.softmax(logits_step, dim=-1)\n",
    "\n",
    "        top_p, top_i = torch.topk(probs, top_k)\n",
    "        step_info = [(itos[idx], p.item()) for idx, p in zip(top_i, top_p)]\n",
    "        steps.append(step_info)\n",
    "\n",
    "        next_idx = top_i[0].item()\n",
    "        caption_idx.append(next_idx)\n",
    "        if next_idx == end_idx:\n",
    "            break\n",
    "\n",
    "    rows = []\n",
    "    for pos, token_probs in enumerate(steps, start=1):\n",
    "        row = OrderedDict(pos=pos)\n",
    "        for r, (tok, p) in enumerate(token_probs, start=1):\n",
    "            row[f'token{r}'] = tok\n",
    "            row[f'prob{r}']  = f'{p:.4f}'\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('pos')\n",
    "    print(df.to_string())\n",
    "\n",
    "    words = [itos[i] for i in caption_idx[1:-1]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Selection of the image to be captioned\n",
    "test_img = 'data/images/val2017/000000579635.jpg'\n",
    "print(caption_with_table(test_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2d535",
   "metadata": {},
   "source": [
    "## 1.2. Pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e19fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMNET_MEAN, IMNET_STD = [0.485,0.456,0.406], [0.229,0.224,0.225] # Imagenet mean and std, resnet is trained on imagenet\n",
    "\n",
    "#Transforms for data\n",
    "tr_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9,1.0), ratio=(0.9,1.1)),\n",
    "    transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(), transforms.Normalize(IMNET_MEAN, IMNET_STD)])\n",
    "va_tf = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),  transforms.Normalize(IMNET_MEAN, IMNET_STD)])\n",
    "\n",
    "#Training dataset and dataloader for pretrained model\n",
    "img_dir='data/images/train2017'\n",
    "train_dataset_pretrained = CaptionImageDataset(\n",
    "    img_dir=img_dir,\n",
    "    annotations=numerical,\n",
    "    vocab=vocab,\n",
    "    transform=tr_tf\n",
    ")\n",
    "train_loader_pretrained = DataLoader(\n",
    "    train_dataset_pretrained,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "#Validation dataset and dataloader for pretrained model\n",
    "val_img_dir='data/images/val2017'\n",
    "val_dataset_pretrained = CaptionImageDataset(\n",
    "    img_dir=val_img_dir,\n",
    "    annotations=numerical_val,\n",
    "    vocab=vocab,\n",
    "    transform=va_tf\n",
    ")\n",
    "val_loader_pretrained = DataLoader(\n",
    "    val_dataset_pretrained,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Positional encoding for 2D images\n",
    "class PE2D(nn.Module):\n",
    "    def __init__(self, d, h=7, w=7):\n",
    "        super().__init__()\n",
    "        if d % 4: raise ValueError(\"d_model must be divisible by 4\")\n",
    "        pe = torch.zeros(d, h, w)\n",
    "        d_half = d // 2\n",
    "        div = torch.exp(torch.arange(0, d_half, 2) * (-math.log(10000.0) / d_half))\n",
    "\n",
    "        pos_w = torch.arange(w).unsqueeze(1)\n",
    "        pos_h = torch.arange(h).unsqueeze(1)\n",
    "\n",
    "        sin_w = torch.sin(pos_w * div).T.unsqueeze(1).repeat(1, h, 1)\n",
    "        cos_w = torch.cos(pos_w * div).T.unsqueeze(1).repeat(1, h, 1)\n",
    "        pe[0:d_half:2]  = sin_w\n",
    "        pe[1:d_half:2]  = cos_w\n",
    "\n",
    "        sin_h = torch.sin(pos_h * div).T.unsqueeze(2).repeat(1, 1, w)\n",
    "        cos_h = torch.cos(pos_h * div).T.unsqueeze(2).repeat(1, 1, w)\n",
    "        pe[d_half::2]   = sin_h\n",
    "        pe[d_half+1::2] = cos_h\n",
    "\n",
    "        self.register_buffer('pe', pe.flatten(1).T.unsqueeze(1))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "# Encoder for pretrained model\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, d=512):\n",
    "        super().__init__()\n",
    "        rn = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*list(rn.children())[:-2])\n",
    "        for p in self.backbone.parameters(): p.requires_grad=False\n",
    "        self.proj = nn.Conv2d(2048, d, 1)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        self.pe2d = PE2D(d, 7, 7)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(self.backbone(x))\n",
    "        x = x.flatten(2).permute(2,0,1)\n",
    "        return self.pe2d(x)\n",
    "\n",
    "# Decoder for pretrained model\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d, heads, d_ff, p=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn  = nn.MultiheadAttention(d, heads, dropout=p, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d, heads, dropout=p, batch_first=True)\n",
    "        self.ff = nn.Sequential(nn.Linear(d, d_ff), nn.ReLU(), nn.Dropout(p),\n",
    "                                nn.Linear(d_ff, d))\n",
    "        self.norm1 = nn.LayerNorm(d); self.drop1 = nn.Dropout(p)\n",
    "        self.norm2 = nn.LayerNorm(d); self.drop2 = nn.Dropout(p)\n",
    "        self.norm3 = nn.LayerNorm(d); self.drop3 = nn.Dropout(p)\n",
    "    def forward(self, x, mem, attn_mask, pad_mask):\n",
    "        sa,_ = self.self_attn(x,x,x, attn_mask=attn_mask, key_padding_mask=pad_mask)\n",
    "        x = self.norm1(x + self.drop1(sa))\n",
    "        ca,_ = self.cross_attn(x, mem, mem)\n",
    "        x = self.norm2(x + self.drop2(ca))\n",
    "        x = self.norm3(x + self.drop3(self.ff(x)))\n",
    "        return x\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d=512, heads=8, d_ff=2048,\n",
    "                 layers=6, dropout=0.1, max_len=70):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab, d)\n",
    "        self.pos = nn.Embedding(max_len, d)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            DecoderBlock(d, heads, d_ff, dropout) for _ in range(layers))\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "        self.out = nn.Linear(d, vocab)\n",
    "        self.d = d\n",
    "    def forward(self, mem, caps, pad=0):\n",
    "        B,L = caps.size()\n",
    "        pos = torch.arange(L-1, device=caps.device).unsqueeze(0)\n",
    "        x = self.tok(caps[:,:-1]) * math.sqrt(self.d) + self.pos(pos)\n",
    "        tgt_mask = torch.triu(torch.ones((L-1, L-1),\n",
    "                                 dtype=torch.bool,\n",
    "                                 device=caps.device), 1)\n",
    "        kpm = (caps[:,:-1] == pad)\n",
    "        mem = mem.permute(1,0,2)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mem, tgt_mask, kpm)\n",
    "        x = self.ln(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a83cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] Train:   0%|          | 3/9012 [00:19<16:32:22,  6.61s/it, lastLoss=9.3437, lr=1.68e-11, meanLoss=9.3498]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     logits \u001b[38;5;241m=\u001b[39m decoder(mem, caps)\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitos\u001b[39m\u001b[38;5;124m'\u001b[39m])), caps[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 39\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(decoder\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr_encoder = 1e-3\n",
    "lr_decoder = 4e-3\n",
    "embed_size_pretrained = 512\n",
    "warmup_steps = 100000\n",
    "EPOCHS = 30\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the models\n",
    "encoder = ResNetEncoder(embed_size_pretrained).to(device)\n",
    "decoder = CaptionDecoder(len(vocab['itos']), embed_size_pretrained).to(device)\n",
    "\n",
    "# Define the loss function (cross entropy)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'], label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decoder.parameters(),          'lr': lr_decoder},\n",
    "    {'params': encoder.proj.parameters(),     'lr': lr_encoder},\n",
    "], weight_decay=1e-4)\n",
    "def transformer_sched(opt, d=embed_size_pretrained, warm=warmup_steps):\n",
    "    f = lambda s: (d**-0.5) * min(s**-0.5, s*warm**-1.5) if s else 0.\n",
    "    return torch.optim.lr_scheduler.LambdaLR(opt, f)\n",
    "scheduler = transformer_sched(optimizer)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Training loop\n",
    "best = float('inf')\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    encoder.train(); decoder.train()\n",
    "    tloss = 0\n",
    "    pbar = tqdm(train_loader_pretrained, desc=f\"[Epoch {ep}/{EPOCHS}] Train\")\n",
    "    for imgs,caps,_ in pbar:\n",
    "        imgs,caps = imgs.to(device), caps.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device.type, enabled=device.type=='cuda'):\n",
    "            mem = encoder(imgs)\n",
    "            logits = decoder(mem, caps)\n",
    "            loss = criterion(logits.reshape(-1,len(vocab['itos'])), caps[:,1:].reshape(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        tloss += batch_loss*imgs.size(0)\n",
    "        pbar.set_postfix(lastLoss=f\"{batch_loss:.4f}\", meanLoss=f\"{tloss/((pbar.n+1)*imgs.size(0)):.4f}\", lr=f\"{current_lr:.2e}\")\n",
    "\n",
    "    tloss /= len(train_loader_pretrained.dataset)\n",
    "\n",
    "    encoder.eval(); decoder.eval()\n",
    "    vloss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs,caps,_ in tqdm(val_loader_pretrained, desc=f'ep{ep} val'):\n",
    "            imgs,caps = imgs.to(device), caps.to(device)\n",
    "            with torch.autocast(device.type, enabled=device.type=='cuda'):\n",
    "                mem = encoder(imgs)\n",
    "                logits = decoder(mem, caps)\n",
    "                loss = criterion(logits.reshape(-1,len(vocab['itos'])), caps[:,1:].reshape(-1))\n",
    "            vloss += loss.item()*imgs.size(0)\n",
    "    vloss /= len(val_loader_pretrained.dataset)\n",
    "\n",
    "    print(f'→ epoch {ep}: train {tloss:.4f} | val {vloss:.4f}')\n",
    "    if vloss < best:\n",
    "        best = vloss\n",
    "        torch.save({'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'itos': itos,\n",
    "            'stoi': stoi},\n",
    "           'best_model_pretrained.pth')\n",
    "        print(f\"best_model_pretrained updated (Val Loss: {best:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b01f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token1   prob1  token2   prob2  token3   prob3   token4   prob4    token5   prob5\n",
      "pos                                                                                       \n",
      "1             a  0.7212     two  0.0433     the  0.0344    there  0.0125        an  0.0114\n",
      "2           man  0.2261  person  0.1628   young  0.0875      boy  0.0468     woman  0.0436\n",
      "3            in  0.1052      is  0.0950  riding  0.0767  holding  0.0495  standing  0.0487\n",
      "4             a  0.5020     the  0.1028   front  0.0388       an  0.0278       his  0.0272\n",
      "5    skateboard  0.2074   skate  0.0372    park  0.0176    field  0.0167      suit  0.0138\n",
      "6            on  0.1010      in  0.0948    with  0.0694       is  0.0522        at  0.0298\n",
      "7             a  0.6152     the  0.0826     top  0.0612      his  0.0509        an  0.0089\n",
      "8    skateboard  0.5967   skate  0.0232   field  0.0141     ramp  0.0136     trick  0.0104\n",
      "9         <end>  0.2590      in  0.1114      on  0.0426     with  0.0383        at  0.0264\n",
      "\n",
      "Caption: a man in a skateboard on a skateboard\n"
     ]
    }
   ],
   "source": [
    "embed_size_pretrained = 512\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create the models\n",
    "encoder = ResNetEncoder(embed_size_pretrained).to(device)\n",
    "decoder = CaptionDecoder(len(vocab['itos']), embed_size_pretrained).to(device)\n",
    "\n",
    "# Load best models\n",
    "ckpt = torch.load('best_model_pretrained.pth', map_location=device)\n",
    "encoder.load_state_dict(ckpt['encoder'])\n",
    "decoder.load_state_dict(ckpt['decoder'])\n",
    "encoder.eval(); decoder.eval()\n",
    "\n",
    "\n",
    "infer_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Function to create a caption from an image\n",
    "@torch.no_grad()\n",
    "def caption_with_table(img_path, max_len=70, top_k=10, temperature=1.0):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = infer_tf(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.amp.autocast(device.type):\n",
    "        feat = encoder(img)\n",
    "\n",
    "    caption_idx = [stoi['<start>']]\n",
    "    steps = []\n",
    "    for _ in range(max_len - 1):\n",
    "        seq = torch.tensor(caption_idx + [stoi['<pad>']],\n",
    "                           device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.amp.autocast(device.type):\n",
    "            logits = decoder(feat, seq)\n",
    "\n",
    "        logits = logits[0, -1] / temperature\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        top_p, top_i = torch.topk(probs, top_k)\n",
    "\n",
    "        steps.append([(itos[i], p.item()) for i, p in zip(top_i, top_p)])\n",
    "        next_idx = top_i[0].item()\n",
    "        caption_idx.append(next_idx)\n",
    "\n",
    "        if next_idx == stoi['<end>']:\n",
    "            break\n",
    "\n",
    "    rows = []\n",
    "    for pos, token_probs in enumerate(steps, start=1):\n",
    "        row = OrderedDict(pos=pos)\n",
    "        for r, (tok, p) in enumerate(token_probs, start=1):\n",
    "            row[f'token{r}'] = tok\n",
    "            row[f'prob{r}']  = f'{p:.4f}'\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('pos')\n",
    "    print(df.to_string())\n",
    "\n",
    "    words = [itos[i] for i in caption_idx[1:-1]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Execution of the function to generate captions\n",
    "test_img = 'data/images/val2017/000000149375.jpg'\n",
    "sent = caption_with_table(test_img, top_k=5)\n",
    "print('\\nCaption:', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07481e0a",
   "metadata": {},
   "source": [
    "# 2. USING LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd88e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep1 train: 100%|██████████| 9247/9247 [27:53<00:00,  5.53it/s, loss=9.1759]  \n",
      "ep1 val: 100%|██████████| 391/391 [00:50<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ epoch 1: train 0.1438 | val 9.1664\n",
      "   ✓ checkpoint guardado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep2 train: 100%|██████████| 9247/9247 [28:50<00:00,  5.34it/s, loss=9.0985]  \n",
      "ep2 val: 100%|██████████| 391/391 [00:56<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ epoch 2: train 0.1427 | val 9.1018\n",
      "   ✓ checkpoint guardado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep3 train: 100%|██████████| 9247/9247 [29:34<00:00,  5.21it/s, loss=9.0274]  \n",
      "ep3 val: 100%|██████████| 391/391 [00:55<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ epoch 3: train 0.1417 | val 9.0318\n",
      "   ✓ checkpoint guardado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep4 train: 100%|██████████| 9247/9247 [29:36<00:00,  5.20it/s, loss=8.9311]  \n",
      "ep4 val: 100%|██████████| 391/391 [00:59<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ epoch 4: train 0.1405 | val 8.9533\n",
      "   ✓ checkpoint guardado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep5 train:   2%|▏         | 149/9247 [00:33<33:37,  4.51it/s, loss=8.9647] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 209\u001b[0m\n\u001b[1;32m    207\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     mem \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     logits \u001b[38;5;241m=\u001b[39m decoder(mem, caps)\n\u001b[1;32m    211\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,V), caps[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 141\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):                      \u001b[38;5;66;03m# [B,3,224,224]\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)        \u001b[38;5;66;03m# [B,D,7,7]\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)        \u001b[38;5;66;03m# [49,B,D]\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe2d(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COCO-Captioner 2025\n",
    "CNN-Transformer con multi-head attention explícito\n",
    "(Python 3.10, PyTorch ≥ 2.1)\n",
    "\n",
    "Pasos previos:\n",
    "  ├─ data/images/train2017/   (118k jpg)\n",
    "  ├─ data/images/val2017/     (5k jpg)\n",
    "  └─ data/annotations/\n",
    "         captions_train2017.json\n",
    "         captions_val2017.json\n",
    "\"\"\"\n",
    "\n",
    "# ---------- 0) Imports ----------\n",
    "import os, re, json, math\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# ---------- 1) Utilidades de texto ----------\n",
    "def tokenize_english(text: str):\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text.lower())\n",
    "    text = re.sub(r\"[^a-z0-9\\.\\,\\!\\?\\:\\;\\’\\'\\-]\", \" \", text)\n",
    "    return [t for t in nltk.word_tokenize(text) if any(c.isalnum() for c in t)]\n",
    "\n",
    "# ---------- 2) Cargar captions ----------\n",
    "def load_coco(split):\n",
    "    with open(f'data/annotations/captions_{split}2017.json') as f:\n",
    "        coco = json.load(f)\n",
    "    id2fname = {img['id']: img['file_name'] for img in coco['images']}\n",
    "    anns = []\n",
    "    for ann in coco['annotations']:\n",
    "        seq = ['<start>'] + tokenize_english(ann['caption']) + ['<end>']\n",
    "        anns.append((id2fname[ann['image_id']], seq))\n",
    "    return anns\n",
    "\n",
    "train_anns = load_coco('train')\n",
    "val_anns   = load_coco('val')\n",
    "\n",
    "# ---------- 3) Vocabulario ----------\n",
    "min_freq = 5\n",
    "counter   = Counter(tok for _, seq in train_anns for tok in seq)\n",
    "special   = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "itos      = special + [w for w, c in counter.items() if c >= min_freq]\n",
    "stoi      = {w: i for i, w in enumerate(itos)}\n",
    "V         = len(itos)\n",
    "\n",
    "def numericalise(anns):\n",
    "    out = []\n",
    "    for fname, seq in anns:\n",
    "        out.append((fname, [stoi.get(t, stoi['<unk>']) for t in seq]))\n",
    "    return out\n",
    "\n",
    "train_nums, val_nums = map(numericalise, (train_anns, val_anns))\n",
    "\n",
    "# ---------- 4) Dataset ----------\n",
    "class CocoDS(Dataset):\n",
    "    def __init__(self, root, pairs, tf):\n",
    "        self.root, self.pairs, self.tf = root, pairs, tf\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        fname, ids = self.pairs[idx]\n",
    "        img = Image.open(os.path.join(self.root, fname)).convert('RGB')\n",
    "        return self.tf(img), torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, seqs = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    Ls   = [len(s) for s in seqs]\n",
    "    Lmax = max(Ls)\n",
    "    padded = torch.full((len(seqs), Lmax), stoi['<pad>'], dtype=torch.long)\n",
    "    for i,s in enumerate(seqs):\n",
    "        padded[i,:Ls[i]] = s\n",
    "    return imgs, padded, Ls\n",
    "\n",
    "# ---------- 5) Transforms & Loaders ----------\n",
    "IMNET_MEAN, IMNET_STD = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "tr_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9,1.0), ratio=(0.9,1.1)),\n",
    "    transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(), transforms.Normalize(IMNET_MEAN, IMNET_STD)])\n",
    "va_tf = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),  transforms.Normalize(IMNET_MEAN, IMNET_STD)])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    CocoDS('data/images/train2017', train_nums, tr_tf),\n",
    "    batch_size=64, shuffle=True,  num_workers=8, pin_memory=True, collate_fn=collate)\n",
    "val_loader = DataLoader(\n",
    "    CocoDS('data/images/val2017', val_nums, va_tf),\n",
    "    batch_size=64, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "# ---------- 6) Encoder (49 tokens + PosEnc 2-D) ----------\n",
    "class PE2D(nn.Module):\n",
    "    def __init__(self, d, h=7, w=7):\n",
    "        super().__init__()\n",
    "        if d % 4: raise ValueError(\"d_model debe ser múltiplo de 4\")\n",
    "        pe = torch.zeros(d, h, w)\n",
    "        d_half = d // 2\n",
    "        div = torch.exp(torch.arange(0, d_half, 2) * (-math.log(10000.0) / d_half))\n",
    "\n",
    "        pos_w = torch.arange(w).unsqueeze(1)                 # [W,1]\n",
    "        pos_h = torch.arange(h).unsqueeze(1)                 # [H,1]\n",
    "\n",
    "        # Ancho\n",
    "        sin_w = torch.sin(pos_w * div).T.unsqueeze(1).repeat(1, h, 1)  # [d/4,H,W]\n",
    "        cos_w = torch.cos(pos_w * div).T.unsqueeze(1).repeat(1, h, 1)\n",
    "        pe[0:d_half:2]  = sin_w\n",
    "        pe[1:d_half:2]  = cos_w\n",
    "\n",
    "        # Alto\n",
    "        sin_h = torch.sin(pos_h * div).T.unsqueeze(2).repeat(1, 1, w)  # [d/4,H,W]\n",
    "        cos_h = torch.cos(pos_h * div).T.unsqueeze(2).repeat(1, 1, w)\n",
    "        pe[d_half::2]   = sin_h\n",
    "        pe[d_half+1::2] = cos_h\n",
    "\n",
    "        self.register_buffer('pe', pe.flatten(1).T.unsqueeze(1))   # [49,1,D]\n",
    "    def forward(self, x):      # x [49,B,D]\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, d=512):\n",
    "        super().__init__()\n",
    "        rn = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*list(rn.children())[:-2])   # hasta conv5_x\n",
    "        for p in self.backbone.parameters(): p.requires_grad=False\n",
    "        self.proj = nn.Conv2d(2048, d, 1)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        self.pe2d = PE2D(d, 7, 7)\n",
    "    def forward(self, x):                      # [B,3,224,224]\n",
    "        x = self.proj(self.backbone(x))        # [B,D,7,7]\n",
    "        x = x.flatten(2).permute(2,0,1)        # [49,B,D]\n",
    "        return self.pe2d(x)\n",
    "\n",
    "# ---------- 7) Decoder con nn.MultiheadAttention ----------\n",
    "class CaptionDecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    *   Imagen → vector (media de los 49 parches)  \n",
    "    *   Ese vector inicializa h0; c0 = 0  \n",
    "    *   Entrada al LSTM: embeddings de palabras desplazados (sin la última)  \n",
    "    *   Salida: logits sobre el vocabulario para cada paso\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d=512, hidden=512,\n",
    "                 layers=2, dropout=0.1, max_len=70):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d)\n",
    "        self.lstm = nn.LSTM(d, hidden, layers,\n",
    "                            dropout=dropout if layers > 1 else 0,\n",
    "                            batch_first=True)\n",
    "        self.h_init = nn.Linear(d, hidden)   # imagen → h0\n",
    "        self.out = nn.Linear(hidden, vocab_size)\n",
    "        self.layers = layers\n",
    "        self.hidden = hidden\n",
    "\n",
    "    def forward(self, mem, caps, pad_idx=0):    # mem [49,B,D]   caps [B,L]\n",
    "        B, L = caps.size()\n",
    "        context = mem.mean(0)                   # [B,D]\n",
    "\n",
    "        h0 = torch.tanh(self.h_init(context))   # [B,H]\n",
    "        h0 = h0.unsqueeze(0).repeat(self.layers, 1, 1)     # [L,B,H]\n",
    "        c0 = torch.zeros_like(h0)                              # igual que h0\n",
    "\n",
    "        emb = self.tok(caps[:, :-1])            # [B,L-1,E]\n",
    "        out, _ = self.lstm(emb, (h0, c0))       # [B,L-1,H]\n",
    "        out = self.out(out)                     # [B,L-1,V]\n",
    "        return out\n",
    "\n",
    "# ---------- 8) Entrenamiento ----------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "D_MODEL = 512\n",
    "encoder = ResNetEncoder(D_MODEL).to(device)\n",
    "decoder = CaptionDecoderLSTM(V, D_MODEL).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'], label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decoder.parameters(),          'lr': 1e-4},\n",
    "    {'params': encoder.proj.parameters(),     'lr': 5e-5},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "def transformer_sched(opt, d=D_MODEL, warm=100000):\n",
    "    f = lambda s: (d**-0.5) * min(s**-0.5, s*warm**-1.5) if s else 0.\n",
    "    return torch.optim.lr_scheduler.LambdaLR(opt, f)\n",
    "scheduler = transformer_sched(optimizer)\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "EPOCHS = 20\n",
    "best = float('inf')\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # ---- train ----\n",
    "    encoder.train(); decoder.train()\n",
    "    tloss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'ep{ep} train')\n",
    "    running_loss = 0.0\n",
    "    for i, (imgs,caps,_) in enumerate(pbar):\n",
    "        imgs,caps = imgs.to(device), caps.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device.type, enabled=device.type=='cuda'):\n",
    "            mem = encoder(imgs)\n",
    "            logits = decoder(mem, caps)\n",
    "            loss = criterion(logits.reshape(-1,V), caps[:,1:].reshape(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        tloss += batch_loss\n",
    "\n",
    "\n",
    "        pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n",
    "    tloss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- val ----\n",
    "    encoder.eval(); decoder.eval()\n",
    "    vloss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs,caps,_ in tqdm(val_loader, desc=f'ep{ep} val'):\n",
    "            imgs,caps = imgs.to(device), caps.to(device)\n",
    "            with torch.autocast(device.type, enabled=device.type=='cuda'):\n",
    "                mem = encoder(imgs)\n",
    "                logits = decoder(mem, caps)\n",
    "                loss = criterion(logits.reshape(-1,V), caps[:,1:].reshape(-1))\n",
    "            vloss += loss.item()*imgs.size(0)\n",
    "    vloss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'→ epoch {ep}: train {tloss:.4f} | val {vloss:.4f}')\n",
    "    if vloss < best:\n",
    "        best = vloss\n",
    "        torch.save({'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'itos': itos,\n",
    "            'stoi': stoi},\n",
    "           'best_model_lstm.pth')\n",
    "        print('   ✓ checkpoint guardado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b80f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "num_heads  = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 6\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "checkpoint_path = 'best_model.pth'\n",
    "image_path      = 'data/images/val2017/000000000139.jpg'  # cambia a tu imagen\n",
    "\n",
    "# 2) Carga vocabulario\n",
    "ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "itos = ckpt['itos']\n",
    "stoi = ckpt['stoi']\n",
    "pad_idx   = stoi['<pad>']\n",
    "start_idx = stoi['<start>']\n",
    "end_idx   = stoi['<end>']\n",
    "\n",
    "# 3) Reconstruye y carga modelos\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = SimpleEncoderCNN(embed_size).to(device)\n",
    "decoder = TransformerDecoder(embed_size, num_heads, hidden_dim, len(itos), num_layers).to(device)\n",
    "\n",
    "state = torch.load(checkpoint_path, map_location=device)\n",
    "encoder.load_state_dict(state['encoder_state'])\n",
    "decoder.load_state_dict(state['decoder_state'])\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 4) Transform para inferencia\n",
    "infer_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# 5) Generación greedy\n",
    "@torch.no_grad()\n",
    "def generate_caption(img_tensor, max_len=50):\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)            # [1,3,H,W]\n",
    "    feat = encoder(img_tensor)                                 # [1, E]\n",
    "    caption = [stoi['<start>']]\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        seq = torch.tensor([caption + [pad_idx]], device=device)\n",
    "        logits = decoder(feat, seq)                           # [1, L, V]\n",
    "        next_logits = logits[0, -1] / 1.0                     # temperatura=1\n",
    "        prob = torch.softmax(next_logits, dim=-1)\n",
    "        next_idx = prob.argmax().item()\n",
    "        caption.append(next_idx)\n",
    "        if next_idx == end_idx:\n",
    "            break\n",
    "\n",
    "    # convierte índices a tokens, quitando <start> y <end>\n",
    "    tokens = [itos[i] for i in caption[1:-1]]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 6) Ejecución\n",
    "if __name__ == '__main__':\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = infer_tf(img)\n",
    "    result = generate_caption(img)\n",
    "    print(\"Generated caption:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
