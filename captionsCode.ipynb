{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a36123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# Third-party library imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ca9013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "#This code is needed for all the models\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "def tokenize_english(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\.\\,\\!\\?\\:\\;\\â€™\\'\\-]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if any(c.isalnum() for c in t)]\n",
    "    return tokens\n",
    "\n",
    "with open('data/annotations/captions_train2017.json', 'r') as f:\n",
    "    coco_train = json.load(f)\n",
    "\n",
    "with open('data/annotations/captions_val2017.json', 'r') as f:\n",
    "    coco_val = json.load(f)\n",
    "\n",
    "img_id_to_filename = {img['id']: img['file_name'] for img in coco_train['images']}\n",
    "\n",
    "annotations = []\n",
    "for ann in coco_train['annotations']:\n",
    "    fname = img_id_to_filename[ann['image_id']]\n",
    "    tokens = tokenize_english(ann['caption'])\n",
    "    seq = ['<start>'] + tokens + ['<end>']\n",
    "    annotations.append((fname, seq))\n",
    "\n",
    "\n",
    "min_freq = 5\n",
    "counter = Counter(tok for _, seq in annotations for tok in seq)\n",
    "words = [w for w, cnt in counter.items() if cnt >= min_freq]\n",
    "specials = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "itos = specials + words\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab = {'itos': itos, 'stoi': stoi}\n",
    "\n",
    "numerical = []\n",
    "for fname, seq in annotations:\n",
    "    ids = [stoi.get(tok, stoi['<unk>']) for tok in seq]\n",
    "    numerical.append((fname, ids))\n",
    "\n",
    "\n",
    "img_id_to_filename_val = {img['id']: img['file_name'] for img in coco_val['images']}\n",
    "\n",
    "annotations_val = []\n",
    "for ann in coco_val['annotations']:\n",
    "    fname = img_id_to_filename_val[ann['image_id']]\n",
    "    tokens = tokenize_english(ann['caption'])\n",
    "    seq = ['<start>'] + tokens + ['<end>']\n",
    "    annotations_val.append((fname, seq))\n",
    "\n",
    "numerical_val = []\n",
    "for fname, seq in annotations_val:\n",
    "    ids = [stoi.get(tok, stoi['<unk>']) for tok in seq]\n",
    "    numerical_val.append((fname, ids))\n",
    "\n",
    "\n",
    "class CaptionImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, vocab, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, seq_ids = self.annotations[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(seq_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, seqs = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    lengths = [len(s) for s in seqs]\n",
    "    max_len = max(lengths)\n",
    "    padded = torch.zeros(len(seqs), max_len, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :lengths[i]] = s\n",
    "    return imgs, padded, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfef3b8",
   "metadata": {},
   "source": [
    "# 1. NO PRETRAINED CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53d90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset that will only be used for computing mean and std\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, fname)\n",
    "            for fname in os.listdir(img_dir)\n",
    "            if fname.lower().endswith(('.jpg', '.png'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Function to compute mean and std\n",
    "def compute_mean_std(img_dir, batch_size=32, num_workers=8):\n",
    "    basic_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset_mean = SimpleImageDataset(img_dir, transform=basic_transform)\n",
    "    loader_mean = DataLoader(dataset_mean, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=num_workers,\n",
    "                        pin_memory=True)\n",
    "\n",
    "    sum_rgb = torch.zeros(3)\n",
    "    sum_squared = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    for batch in loader_mean:\n",
    "        b, c, h, w = batch.shape\n",
    "        sum_rgb += batch.sum(dim=[0, 2, 3])\n",
    "        sum_squared += (batch ** 2).sum(dim=[0, 2, 3])\n",
    "        num_pixels += b * h * w\n",
    "\n",
    "    mean = sum_rgb / num_pixels\n",
    "    std = torch.sqrt((sum_squared / num_pixels) - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "# Compute mean and std for the training images and create the datasets and loaders\n",
    "img_directory = 'data/images/train2017'\n",
    "if os.path.exists('image_stats.pkl'):\n",
    "    with open('image_stats.pkl', 'rb') as f:\n",
    "        stats = pickle.load(f)\n",
    "        mean, std = stats['mean'], stats['std']\n",
    "else:\n",
    "    mean, std = compute_mean_std(img_directory)\n",
    "    stats = {'mean': mean, 'std': std}\n",
    "    with open('image_stats.pkl', 'wb') as f:\n",
    "        pickle.dump(stats, f)\n",
    "\n",
    "train_transform_no_pretrained = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(\n",
    "        256, \n",
    "        scale=(0.9, 1.0), \n",
    "        ratio=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.1, \n",
    "        contrast=0.1, \n",
    "        saturation=0.1, \n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "dataset_no_pretrained = CaptionImageDataset(\n",
    "    img_dir=img_directory,\n",
    "    annotations=numerical,\n",
    "    vocab=vocab,\n",
    "    transform=train_transform_no_pretrained\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader_no_pretrained = DataLoader(\n",
    "    dataset_no_pretrained,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_transform_no_pretrained = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "val_img_directory = 'data/images/val2017'\n",
    "\n",
    "val_dataset_no_pretrained = CaptionImageDataset(\n",
    "    img_dir=val_img_directory,\n",
    "    annotations=numerical_val,\n",
    "    vocab=vocab,\n",
    "    transform=val_transform_no_pretrained\n",
    ")\n",
    "\n",
    "val_loader_no_pretrained = DataLoader(\n",
    "    val_dataset_no_pretrained,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Define the models\n",
    "# Encoder\n",
    "class SimpleEncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SimpleEncoderCNN, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.embed = nn.Linear(512, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embed(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "    \n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_dim, vocab_size, max_len=70, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_size),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def _generate_square_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        tgt = captions[:, :-1]  \n",
    "        x = self.embed(tgt) * math.sqrt(self.embed.embedding_dim)\n",
    "        x = self.pos_enc(x)\n",
    "        x = x.transpose(0, 1) \n",
    "\n",
    "        mask = self._generate_square_mask(x.size(0), x.device)\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        mem = features.unsqueeze(0)           \n",
    "        cross_out, _ = self.cross_attn(x, mem, mem)\n",
    "        x = self.norm2(x + self.dropout(cross_out))\n",
    "\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "\n",
    "        x = x.transpose(0, 1)      \n",
    "        logits = self.out(x)                                \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4480b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/4] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [40:44<00:00,  7.37it/s, lastLoss=3.9643, lr=2.82e-06, meanLoss=6.2744]\n",
      "[Epoch 1/4] Val  : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:48<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 6.2744, Val Loss = 4.5102\n",
      "best_model_no_pretrained updated (Val Loss: 4.5102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/4] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [40:59<00:00,  7.33it/s, lastLoss=2.8937, lr=2.33e-06, meanLoss=4.2410]\n",
      "[Epoch 2/4] Val  : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 4.2410, Val Loss = 4.0112\n",
      "best_model_no_pretrained updated (Val Loss: 4.0112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/4] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [40:57<00:00,  7.33it/s, lastLoss=3.8818, lr=1.90e-06, meanLoss=3.9599]\n",
      "[Epoch 3/4] Val  : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 3.9599, Val Loss = 3.8433\n",
      "best_model_no_pretrained updated (Val Loss: 3.8433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/4] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [41:05<00:00,  7.31it/s, lastLoss=3.3516, lr=1.65e-06, meanLoss=3.8348]\n",
      "[Epoch 4/4] Val  : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:48<00:00, 16.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 3.8348, Val Loss = 3.7478\n",
      "best_model_no_pretrained updated (Val Loss: 3.7478)\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "embed_size  = 512\n",
    "num_heads   = 8\n",
    "hidden_dim  = 2048\n",
    "num_epochs = 4\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-4\n",
    "warmup_steps = 20000\n",
    "\n",
    "vocab_size  = len(vocab['itos'])\n",
    "#Load the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Model\n",
    "encoder = SimpleEncoderCNN(embed_size).to(device)\n",
    "decoder = TransformerDecoder(\n",
    "    embed_size, num_heads, hidden_dim, vocab_size\n",
    ").to(device)\n",
    "\n",
    "#Loss function, optimizer and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['stoi']['<pad>'])\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "def get_transformer_scheduler(optimizer, d_model, warmup_steps=1000):\n",
    "    def lr_lambda(step):\n",
    "        step = max(step, 1)\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "scaler = torch.amp.GradScaler(\n",
    "    'cuda',\n",
    "    init_scale=512, \n",
    "    growth_interval=2000,\n",
    "    growth_factor=2.0,\n",
    "    backoff_factor=0.5\n",
    ")\n",
    "scheduler = get_transformer_scheduler(optimizer, embed_size, warmup_steps=warmup_steps)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader_no_pretrained, desc=f\"[Epoch {epoch}/{num_epochs}] Train\")\n",
    "    for images, captions, lengths in pbar:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            feats   = encoder(images)\n",
    "            logits  = decoder(feats, captions)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.float().view(-1, vocab_size),\n",
    "            captions[:, 1:].contiguous().view(-1)\n",
    "        )\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            continue\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            max_norm=0.5\n",
    "        )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix(lastLoss=f\"{loss.item():.4f}\", meanLoss=f\"{train_loss/(pbar.n+1):.4f}\", lr=f\"{lr:.2e}\")\n",
    "\n",
    "    \n",
    "    avg_train = train_loss / len(train_loader_no_pretrained)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    val_loss = 0.0\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in tqdm(val_loader_no_pretrained, desc=f\"[Epoch {epoch}/{num_epochs}] Val  \"):\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                feats   = encoder(images)\n",
    "                outputs = decoder(feats, captions)\n",
    "                loss    = criterion(\n",
    "                    outputs.view(-1, vocab_size),\n",
    "                    captions[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val = val_loss / len(val_loader_no_pretrained)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {avg_train:.4f}, Val Loss = {avg_val:.4f}\")\n",
    "\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'encoder_state': encoder.state_dict(),\n",
    "            'decoder_state': decoder.state_dict(),\n",
    "            'optim_state': optimizer.state_dict(),\n",
    "            'sched_state': scheduler.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(ckpt, 'best_model_no_pretrained.pth')\n",
    "        print(f\"best_model_no_pretrained updated (Val Loss: {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2d535",
   "metadata": {},
   "source": [
    "# 2. Pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e19fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMNET_MEAN, IMNET_STD = [0.485,0.456,0.406], [0.229,0.224,0.225] # Imagenet mean and std, resnet is trained on imagenet\n",
    "\n",
    "#Transforms for data\n",
    "tr_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9,1.0), ratio=(0.9,1.1)),\n",
    "    transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(), transforms.Normalize(IMNET_MEAN, IMNET_STD)])\n",
    "va_tf = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),  transforms.Normalize(IMNET_MEAN, IMNET_STD)])\n",
    "\n",
    "#Training dataset and dataloader for pretrained model\n",
    "img_dir='data/images/train2017'\n",
    "train_dataset_pretrained = CaptionImageDataset(\n",
    "    img_dir=img_dir,\n",
    "    annotations=numerical,\n",
    "    vocab=vocab,\n",
    "    transform=tr_tf\n",
    ")\n",
    "train_loader_pretrained = DataLoader(\n",
    "    train_dataset_pretrained,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "#Validation dataset and dataloader for pretrained model\n",
    "val_img_dir='data/images/val2017'\n",
    "val_dataset_pretrained = CaptionImageDataset(\n",
    "    img_dir=val_img_dir,\n",
    "    annotations=numerical_val,\n",
    "    vocab=vocab,\n",
    "    transform=va_tf\n",
    ")\n",
    "val_loader_pretrained = DataLoader(\n",
    "    val_dataset_pretrained,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Positional encoding for 2D images\n",
    "class PE2D(nn.Module):\n",
    "    def __init__(self, d, h=7, w=7):\n",
    "        super().__init__()\n",
    "        if d % 4: raise ValueError(\"d_model must be divisible by 4\")\n",
    "        pe = torch.zeros(d, h, w)\n",
    "        d_half = d // 2\n",
    "        div = torch.exp(torch.arange(0, d_half, 2) * (-math.log(10000.0) / d_half))\n",
    "\n",
    "        pos_w = torch.arange(w).unsqueeze(1)\n",
    "        pos_h = torch.arange(h).unsqueeze(1)\n",
    "\n",
    "        sin_w = torch.sin(pos_w * div).T.unsqueeze(1).repeat(1, h, 1)\n",
    "        cos_w = torch.cos(pos_w * div).T.unsqueeze(1).repeat(1, h, 1)\n",
    "        pe[0:d_half:2]  = sin_w\n",
    "        pe[1:d_half:2]  = cos_w\n",
    "\n",
    "        sin_h = torch.sin(pos_h * div).T.unsqueeze(2).repeat(1, 1, w)\n",
    "        cos_h = torch.cos(pos_h * div).T.unsqueeze(2).repeat(1, 1, w)\n",
    "        pe[d_half::2]   = sin_h\n",
    "        pe[d_half+1::2] = cos_h\n",
    "\n",
    "        self.register_buffer('pe', pe.flatten(1).T.unsqueeze(1))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "# Encoder for pretrained model\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, d=512):\n",
    "        super().__init__()\n",
    "        rn = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*list(rn.children())[:-2])\n",
    "        for p in self.backbone.parameters(): p.requires_grad=False\n",
    "        self.proj = nn.Conv2d(2048, d, 1)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        self.pe2d = PE2D(d, 7, 7)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(self.backbone(x))\n",
    "        x = x.flatten(2).permute(2,0,1)\n",
    "        return self.pe2d(x)\n",
    "\n",
    "# Decoder for pretrained model\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d, heads, d_ff, p=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn  = nn.MultiheadAttention(d, heads, dropout=p, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d, heads, dropout=p, batch_first=True)\n",
    "        self.ff = nn.Sequential(nn.Linear(d, d_ff), nn.ReLU(), nn.Dropout(p),\n",
    "                                nn.Linear(d_ff, d))\n",
    "        self.norm1 = nn.LayerNorm(d); self.drop1 = nn.Dropout(p)\n",
    "        self.norm2 = nn.LayerNorm(d); self.drop2 = nn.Dropout(p)\n",
    "        self.norm3 = nn.LayerNorm(d); self.drop3 = nn.Dropout(p)\n",
    "    def forward(self, x, mem, attn_mask, pad_mask):\n",
    "        sa,_ = self.self_attn(x,x,x, attn_mask=attn_mask, key_padding_mask=pad_mask)\n",
    "        x = self.norm1(x + self.drop1(sa))\n",
    "        ca,_ = self.cross_attn(x, mem, mem)\n",
    "        x = self.norm2(x + self.drop2(ca))\n",
    "        x = self.norm3(x + self.drop3(self.ff(x)))\n",
    "        return x\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab, d=512, heads=8, d_ff=2048,\n",
    "                 layers=6, dropout=0.1, max_len=70):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab, d)\n",
    "        self.pos = nn.Embedding(max_len, d)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            DecoderBlock(d, heads, d_ff, dropout) for _ in range(layers))\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "        self.out = nn.Linear(d, vocab)\n",
    "        self.d = d\n",
    "    def forward(self, mem, caps, pad=0):\n",
    "        B,L = caps.size()\n",
    "        pos = torch.arange(L-1, device=caps.device).unsqueeze(0)\n",
    "        x = self.tok(caps[:,:-1]) * math.sqrt(self.d) + self.pos(pos)\n",
    "        tgt_mask = torch.triu(torch.ones((L-1, L-1),\n",
    "                                 dtype=torch.bool,\n",
    "                                 device=caps.device), 1)\n",
    "        kpm = (caps[:,:-1] == pad)\n",
    "        mem = mem.permute(1,0,2)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mem, tgt_mask, kpm)\n",
    "        x = self.ln(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [27:50<00:00, 10.79it/s, lastLoss=6.2569, lr=2.85e-07, meanLoss=7.4869]\n",
      "ep1 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:42<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 1: train 7.4869 | val 6.2537\n",
      "best_model_pretrained updated (Val Loss: 6.2537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:09<00:00, 10.67it/s, lastLoss=4.8921, lr=5.70e-07, meanLoss=5.6661]\n",
      "ep2 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 2: train 5.6661 | val 5.1725\n",
      "best_model_pretrained updated (Val Loss: 5.1725)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:15<00:00, 10.63it/s, lastLoss=4.9420, lr=7.60e-07, meanLoss=4.9672]\n",
      "ep3 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 3: train 4.9672 | val 4.7133\n",
      "best_model_pretrained updated (Val Loss: 4.7133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:14<00:00, 10.64it/s, lastLoss=5.0846, lr=6.58e-07, meanLoss=4.6383]\n",
      "ep4 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 4: train 4.6383 | val 4.4884\n",
      "best_model_pretrained updated (Val Loss: 4.4884)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:12<00:00, 10.65it/s, lastLoss=3.8941, lr=5.89e-07, meanLoss=4.4642]\n",
      "ep5 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:44<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 5: train 4.4642 | val 4.3503\n",
      "best_model_pretrained updated (Val Loss: 4.3503)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:18<00:00, 10.61it/s, lastLoss=4.7821, lr=5.38e-07, meanLoss=4.3527]\n",
      "ep6 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 6: train 4.3527 | val 4.2603\n",
      "best_model_pretrained updated (Val Loss: 4.2603)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:18<00:00, 10.61it/s, lastLoss=4.6283, lr=4.98e-07, meanLoss=4.2725]\n",
      "ep7 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 7: train 4.2725 | val 4.1905\n",
      "best_model_pretrained updated (Val Loss: 4.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:11<00:00, 10.65it/s, lastLoss=5.3288, lr=4.66e-07, meanLoss=4.2109]\n",
      "ep8 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:42<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 8: train 4.2109 | val 4.1399\n",
      "best_model_pretrained updated (Val Loss: 4.1399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:25<00:00, 10.57it/s, lastLoss=4.5949, lr=4.39e-07, meanLoss=4.1616]\n",
      "ep9 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 9: train 4.1616 | val 4.0950\n",
      "best_model_pretrained updated (Val Loss: 4.0950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10/15] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18024/18024 [28:41<00:00, 10.47it/s, lastLoss=3.9350, lr=4.16e-07, meanLoss=4.1204]\n",
      "ep10 val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:41<00:00, 18.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ epoch 10: train 4.1204 | val 4.0560\n",
      "best_model_pretrained updated (Val Loss: 4.0560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 11/15] Train:  28%|â–ˆâ–ˆâ–Š       | 5106/18024 [08:10<21:06, 10.20it/s, lastLoss=3.8725, lr=4.11e-07, meanLoss=4.0963]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr_encoder = 1e-3\n",
    "lr_decoder = 4e-3\n",
    "embed_size_pretrained = 512\n",
    "warmup_steps = 50000\n",
    "EPOCHS = 15\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the models\n",
    "encoder = ResNetEncoder(embed_size_pretrained).to(device)\n",
    "decoder = CaptionDecoder(len(vocab['itos']), embed_size_pretrained).to(device)\n",
    "\n",
    "# Define the loss function (cross entropy)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'], label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decoder.parameters(),          'lr': lr_decoder},\n",
    "    {'params': encoder.proj.parameters(),     'lr': lr_encoder},\n",
    "], weight_decay=1e-4)\n",
    "def transformer_sched(opt, d=embed_size_pretrained, warm=warmup_steps):\n",
    "    f = lambda s: (d**-0.5) * min(s**-0.5, s*warm**-1.5) if s else 0.\n",
    "    return torch.optim.lr_scheduler.LambdaLR(opt, f)\n",
    "scheduler = transformer_sched(optimizer)\n",
    "scaler = torch.amp.GradScaler(device='cuda')\n",
    "\n",
    "# Training loop\n",
    "best = float('inf')\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    encoder.train(); decoder.train()\n",
    "    tloss = 0\n",
    "    samples_seen = 0\n",
    "    pbar = tqdm(train_loader_pretrained, desc=f\"[Epoch {ep}/{EPOCHS}] Train\")\n",
    "    for imgs,caps,_ in pbar:\n",
    "        imgs,caps = imgs.to(device), caps.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device.type, enabled=device.type=='cuda'):\n",
    "            mem = encoder(imgs)\n",
    "            logits = decoder(mem, caps)\n",
    "            loss = criterion(logits.reshape(-1,len(vocab['itos'])), caps[:,1:].reshape(-1))\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            continue\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        samples_seen += imgs.size(0)\n",
    "\n",
    "        tloss += batch_loss*imgs.size(0)\n",
    "        pbar.set_postfix(lastLoss=f\"{batch_loss:.4f}\", meanLoss=f\"{tloss/(samples_seen):.4f}\", lr=f\"{current_lr:.2e}\")\n",
    "\n",
    "    tloss /= len(train_loader_pretrained.dataset)\n",
    "\n",
    "    encoder.eval(); decoder.eval()\n",
    "    vloss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs,caps,_ in tqdm(val_loader_pretrained, desc=f'ep{ep} val'):\n",
    "            imgs,caps = imgs.to(device), caps.to(device)\n",
    "            with torch.autocast(device.type, enabled=device.type=='cuda'):\n",
    "                mem = encoder(imgs)\n",
    "                logits = decoder(mem, caps)\n",
    "                loss = criterion(logits.reshape(-1,len(vocab['itos'])), caps[:,1:].reshape(-1))\n",
    "            vloss += loss.item()*imgs.size(0)\n",
    "    vloss /= len(val_loader_pretrained.dataset)\n",
    "\n",
    "    print(f'â†’ epoch {ep}: train {tloss:.4f} | val {vloss:.4f}')\n",
    "    if vloss < best:\n",
    "        best = vloss\n",
    "        torch.save({'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'itos': itos,\n",
    "            'stoi': stoi},\n",
    "           'best_model_pretrained.pth')\n",
    "        print(f\"best_model_pretrained updated (Val Loss: {best:.4f})\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37461ed",
   "metadata": {},
   "source": [
    "# EVALUATION OF THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968a2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f12469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotations/captions_test2017.json', 'r') as f:\n",
    "    coco_test = json.load(f)\n",
    "# Function to generate captions for images\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "@torch.no_grad()\n",
    "def generate_caption(img_path, encoder, decoder, infer_tf, table, max_len=70, top_k=1, temperature=1.0):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = infer_tf(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.amp.autocast(device.type):\n",
    "        feat = encoder(img)\n",
    "\n",
    "    caption_idx = [stoi['<start>']]\n",
    "    steps = []\n",
    "    for _ in range(max_len - 1):\n",
    "        seq = torch.tensor(caption_idx + [stoi['<pad>']],\n",
    "                           device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.amp.autocast(device.type):\n",
    "            logits = decoder(feat, seq)\n",
    "\n",
    "        logits = logits[0, -1] / temperature\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        top_p, top_i = torch.topk(probs, top_k)\n",
    "\n",
    "        steps.append([(itos[i], p.item()) for i, p in zip(top_i, top_p)])\n",
    "        next_idx = top_i[0].item()\n",
    "        caption_idx.append(next_idx)\n",
    "\n",
    "        if next_idx == stoi['<end>']:\n",
    "            break\n",
    "        \n",
    "    if table:\n",
    "        rows = []\n",
    "        for pos, token_probs in enumerate(steps, start=1):\n",
    "            row = OrderedDict(pos=pos)\n",
    "            for r, (tok, p) in enumerate(token_probs, start=1):\n",
    "                row[f'token{r}'] = tok\n",
    "                row[f'prob{r}']  = f'{p:.4f}'\n",
    "            rows.append(row)\n",
    "        df = pd.DataFrame(rows).set_index('pos')\n",
    "        print(df.to_string())\n",
    "\n",
    "    words = [itos[i] for i in caption_idx[1:-1]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def evaluate_model(encoder, decoder, test_dataset, transform, itos, stoi, spice=True):\n",
    "    references = {}\n",
    "    hypotheses = {}\n",
    "    \n",
    "    # Get image paths and reference captions\n",
    "    img_paths = []\n",
    "    img_captions = defaultdict(list)\n",
    "    \n",
    "    for ann in test_dataset['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        for img in test_dataset['images']:\n",
    "            if img['id'] == img_id:\n",
    "                img_path = f\"data/images/test2017/{img['file_name']}\"\n",
    "                img_paths.append((img_id, img_path))\n",
    "                caption = ' '.join(tokenize_english(ann['caption']))\n",
    "                img_captions[img_id].append(caption)\n",
    "                break\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_paths = []\n",
    "    seen = set()\n",
    "    for img_id, path in img_paths:\n",
    "        if img_id not in seen:\n",
    "            seen.add(img_id)\n",
    "            unique_paths.append((img_id, path))\n",
    "    \n",
    "    # Generate captions for each image\n",
    "    for i, (img_id, img_path) in enumerate(tqdm(unique_paths)):\n",
    "        try:\n",
    "            generated_caption = generate_caption(img_path, encoder, decoder, transform, table=False)\n",
    "            hypotheses[str(i)] = [generated_caption]\n",
    "            references[str(i)] = [cap for cap in img_captions[img_id]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'BLEU': Bleu(4),\n",
    "        'ROUGE': Rouge(),\n",
    "        'METEOR': Meteor(),\n",
    "        'CIDEr': Cider(),\n",
    "    }\n",
    "    \n",
    "    # Try to use SPICE if available\n",
    "    if spice:\n",
    "        metrics['SPICE'] = Spice()\n",
    "    results = {}\n",
    "    for metric_name, metric in metrics.items():\n",
    "        if metric_name == 'BLEU':\n",
    "            score, _ = metric.compute_score(references, hypotheses)\n",
    "            for i, n in enumerate([1, 2, 3, 4]):\n",
    "                results[f'BLEU-{n}'] = score[i]\n",
    "        else:\n",
    "            score, _ = metric.compute_score(references, hypotheses)\n",
    "            results[metric_name] = score\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42143a",
   "metadata": {},
   "source": [
    "## No pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb0ff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [01:52<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 30108, 'reflen': 26487, 'guess': [30108, 27108, 24108, 21108], 'correct': [12756, 4122, 1171, 300]}\n",
      "ratio: 1.1367085740173997\n",
      "Evaluation results of the model without pretraining:\n",
      "BLEU-1: 0.4237\n",
      "BLEU-2: 0.2538\n",
      "BLEU-3: 0.1463\n",
      "BLEU-4: 0.0817\n",
      "ROUGE: 0.3699\n",
      "METEOR: 0.1265\n",
      "CIDEr: 0.2118\n"
     ]
    }
   ],
   "source": [
    "embed_size_no_pretrained = 512\n",
    "itos = vocab['itos']\n",
    "stoi = vocab['stoi']\n",
    "pad_idx   = stoi['<pad>']\n",
    "start_idx = stoi['<start>']\n",
    "end_idx   = stoi['<end>']\n",
    "num_heads  = 8\n",
    "hidden_dim = 2048\n",
    "# Create the models\n",
    "encoder = SimpleEncoderCNN(embed_size_no_pretrained).to(device)\n",
    "decoder = TransformerDecoder(\n",
    "    embed_size_no_pretrained, num_heads, hidden_dim, len(itos)\n",
    ").to(device)\n",
    "\n",
    "# Load best models\n",
    "ckpt = torch.load('best_model_no_pretrained.pth', map_location=device)\n",
    "encoder.load_state_dict(ckpt['encoder_state'])\n",
    "decoder.load_state_dict(ckpt['decoder_state'])\n",
    "encoder.eval(); decoder.eval()\n",
    "\n",
    "infer_transform_no_pretrained = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "# Evaluate the model\n",
    "results = evaluate_model(encoder, decoder, coco_test, infer_transform_no_pretrained, itos, stoi, False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation results of the model without pretraining:\")\n",
    "for metric, score in results.items():\n",
    "    if isinstance(score, list):\n",
    "        print(f\"{metric}: {', '.join([f'{s:.4f}' for s in score])}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209a130",
   "metadata": {},
   "source": [
    "## Pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [07:31<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 28934, 'reflen': 28719, 'guess': [28934, 25934, 22934, 19934], 'correct': [18662, 8636, 3351, 1230]}\n",
      "ratio: 1.0074863330895572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.753 s\n",
      "Evaluation results for the model that uses ResNet50:\n",
      "BLEU-1: 0.6450\n",
      "BLEU-2: 0.4634\n",
      "BLEU-3: 0.3154\n",
      "BLEU-4: 0.2098\n",
      "ROUGE: 0.4716\n",
      "METEOR: 0.2022\n",
      "CIDEr: 0.6338\n",
      "SPICE: 0.1335\n"
     ]
    }
   ],
   "source": [
    "embed_size_pretrained = 512\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create the models\n",
    "encoder_pretrained = ResNetEncoder(embed_size_pretrained).to(device)\n",
    "decoder_pretrained = CaptionDecoder(len(vocab['itos']), embed_size_pretrained).to(device)\n",
    "\n",
    "# Load best models\n",
    "ckpt = torch.load('best_model_pretrained.pth', map_location=device)\n",
    "encoder_pretrained.load_state_dict(ckpt['encoder'])\n",
    "decoder_pretrained.load_state_dict(ckpt['decoder'])\n",
    "encoder_pretrained.eval(); decoder_pretrained.eval()\n",
    "\n",
    "\n",
    "infer_tf_pretrained = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "results = evaluate_model(\n",
    "    encoder_pretrained,\n",
    "    decoder_pretrained,\n",
    "    coco_test,\n",
    "    infer_tf_pretrained,\n",
    "    itos,\n",
    "    stoi\n",
    ")\n",
    "print(\"Evaluation results for the model that uses ResNet50:\")\n",
    "for metric, score in results.items():\n",
    "    if isinstance(score, list):\n",
    "        print(f\"{metric}: {', '.join([f'{s:.4f}' for s in score])}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
